# -*- coding: utf-8 -*-
"""Heat Project Rough Draft

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Iby_ZrctGSwCzGOC1nfsd8poCp46uUTD
"""

import torch
import numpy as np
import torch.nn as nn
import torch.optim as optim
import matplotlib.pyplot as plt
from scipy.interpolate import interp1d

# Table 2 data
# Times in seconds
times = torch.tensor([0, 30, 60, 90, 120, 180, 240, 300, 360])

# Dimension tests in meters
dim_test_1 = torch.tensor([
    [5.16, 30.15, 39.95], [4.15, 28.38, 37.78], [3.35, 26.78, 36.81],
    [2.80, 25.59, 35.11], [2.45, 24.79, 33.86], [1.71, 24.12, 32.62],
    [1.42, 23.72, 31.85], [1.36, 23.41, 31.32], [1.33, 23.36, 31.24] ]) / 1000

dim_test_2 = torch.tensor([
    [10.15, 30.29, 40.18], [9.27, 28.04, 39.04], [8.52, 27.81, 37.82],
    [7.84, 26.98, 36.92], [7.20, 26.42, 36.31], [6.02, 25.37, 35.22],
    [4.94, 24.33, 34.53], [4.04, 23.75, 33.92], [3.48, 23.31, 33.02] ]) / 1000

dim_test_3 = torch.tensor([
    [15.15, 30.18, 40.12], [13.97, 29.78, 39.42], [12.91, 29.05, 38.48],
    [12.02, 28.14, 37.54], [11.24, 27.48, 36.48], [9.80, 26.52, 35.42],
    [8.62, 25.62, 34.81], [7.31, 24.81, 34.15], [6.84, 24.28, 33.66] ]) / 1000

# Mass data (convert g to kg)
mass_test_1 = torch.tensor([6.80, 5.45, 4.35, 3.43, 2.78, 2.02, 1.54, 1.35, 1.28]) / 1000
mass_test_2 = torch.tensor([14.47, 12.69, 11.14, 9.73, 8.67, 7.36, 6.38, 5.57, 4.87]) / 1000
mass_test_3 = torch.tensor([20.06, 18.12, 16.47, 14.94, 13.75, 11.80, 10.28, 9.03, 7.93]) / 1000

# Volumes (m³)
vol_test_1 = dim_test_1[:, 0] * dim_test_1[:, 1] * dim_test_1[:, 2]
vol_test_2 = dim_test_2[:, 0] * dim_test_2[:, 1] * dim_test_2[:, 2]
vol_test_3 = dim_test_3[:, 0] * dim_test_3[:, 1] * dim_test_3[:, 2]

# Densities (kg/m³)
density_1 = mass_test_1 / vol_test_1
density_2 = mass_test_2 / vol_test_2
density_3 = mass_test_3 / vol_test_3

# Thermal Diffusivity constant estimation
# α = k / (ρ * c) where k is the thermal conductivity, ρ is the density, and c is the specific heat capacity
# With the available data, we can estimate the density ρ and then approximate α

# Assumed values for potato
thermal_conductivity_k = 0.751  # W/m·K
specific_heat_capacity_c = 3400  # J/kg·K

alpha_test_1 = thermal_conductivity_k / (density_1 * specific_heat_capacity_c)
alpha_test_2 = thermal_conductivity_k / (density_2 * specific_heat_capacity_c)
alpha_test_3 = thermal_conductivity_k / (density_3 * specific_heat_capacity_c)

alpha_mean = torch.mean(torch.stack([alpha_test_1, alpha_test_2, alpha_test_3]), dim=0)
print("Mean thermal diffusivity:", alpha_mean)

alpha = torch.mean(alpha_mean)
print("Mean thermal diffusivity:", alpha.item())

# Times in mins, Th/W/L in mm, Temp in °C
table2_data_test3 = {
    "Times": [0, 30, 60, 90, 120, 180, 240, 300, 360, 420, 540, 660, 840, 1020],

    # Test 3
    "Thickness_Test3": [15.15, 13.97, 12.91, 12.02, 11.24, 9.80, 8.62, 7.31, 6.84, 6.18, 5.13, 4.53, 4.32, 4.28],
    "Width_Test3": [30.18, 29.78, 29.05, 28.14, 27.48, 26.52, 25.62, 24.81, 24.28, 23.61, 22.74, 22.18, 22.04, 21.96],
    "T4_Test3": [20.75, 25.22, 25.99, 27.60, 31.88, 34.85, 36.59, 37.43, 38.28, 39.12, 43.30, 46.98, 47.92, 48.59] }

alpha = 1.691709314854961e-07  # thermal diffusivity

# Synthetic Data Generation using the X10 Rule + Noise
# Original data
times_orig = np.array(table2_data_test3["Times"])
thickness_orig = np.array(table2_data_test3["Thickness_Test3"])
width_orig = np.array(table2_data_test3["Width_Test3"])
temp_orig = np.array(table2_data_test3["T4_Test3"])

# Interpolators to predict unknown values within our range
thickness_fn = interp1d(times_orig, thickness_orig, kind= 'cubic')
width_fn = interp1d(times_orig, width_orig, kind= 'cubic')
temp_fn = interp1d(times_orig, temp_orig, kind= 'cubic')

# Generate 10x data
num_synthetic = len(times_orig) * 10
times_synth = np.sort(np.random.uniform(low=times_orig.min(), high=times_orig.max(), size=num_synthetic))

# Noise control (Don't make this too large)
noise_factor = 3

# Apply interpolators
thickness_noise = np.random.normal(0, 0.05 * noise_factor, size=num_synthetic)
width_noise = np.random.normal(0, 0.05 * noise_factor, size=num_synthetic)
temp_noise = np.random.normal(0, 0.2 * noise_factor, size=num_synthetic)

thickness_synth = thickness_fn(times_synth) + thickness_noise
width_synth = width_fn(times_synth) + width_noise
temp_synth = temp_fn(times_synth) + temp_noise

x_train = torch.tensor(width_synth, dtype=torch.float32, requires_grad=True).reshape(-1, 1)
y_train = torch.tensor(thickness_synth, dtype=torch.float32, requires_grad=True).reshape(-1, 1)
t_train = torch.tensor(times_synth, dtype=torch.float32, requires_grad=True).reshape(-1, 1)
temps_tensor = torch.tensor(temp_synth, dtype=torch.float32, requires_grad=True).reshape(-1, 1)

# ICs from t=0
x_ic = torch.tensor([width_orig[0]], dtype=torch.float32).reshape(-1, 1)
y_ic = torch.tensor([thickness_orig[0]], dtype=torch.float32).reshape(-1, 1)
u_ic = torch.tensor([temp_orig[0]], dtype=torch.float32).reshape(-1, 1)

# Dummy BCs
x_bc = x_train.clone()
y_bc = y_train.clone()
u_bc = temps_tensor.clone()

# Define the 2D Heat Equation PINN
class HeatPINN2D(nn.Module):
    def __init__(self):
        super(HeatPINN2D, self).__init__()
        self.model = nn.Sequential(
            nn.Linear(3, 64),  # Inputs: (x, y, t)
            nn.Tanh(),
            nn.Linear(64, 64),
            nn.Tanh(),
            nn.Linear(64, 1)   # Output: temperature u(x, y, t)
        )

    def forward(self, x, y, t):
        inputs = torch.cat((x, y, t), dim=1)
        u = self.model(inputs)
        return u


# Physics-informed loss function
def heat_loss_fn(model, x, y, t, alpha, x_bc, y_bc, u_bc, x_ic, y_ic, u_ic, lambda_reg=0.01, lambda_m=0.01):
    def _grad(out, inp):
        return torch.autograd.grad(out, inp, grad_outputs=torch.ones_like(out), create_graph=True)[0]

    u = model(x, y, t)

    u_t = _grad(u, t)
    u_x = _grad(u, x)
    u_xx = _grad(u_x, x)
    u_y = _grad(u, y)
    u_yy = _grad(u_y, y)

    # 1 PDE residual loss
    loss_eq = torch.mean((u_t - alpha * (u_xx + u_yy)) ** 2)

    # 2 BC loss
    u_bc_pred = model(x_bc, y_bc, t)
    loss_bc = torch.mean((u_bc_pred - u_bc) ** 2)

    # IC loss
    u_ic_pred = model(x_ic, y_ic, torch.zeros_like(x_ic))
    loss_ic = torch.mean((u_ic_pred - u_ic) ** 2)

    # Ridge Regularization (L2 penalty on weights)
    l2_penalty = sum(torch.sum(param ** 2) for param in model.parameters())
    loss_reg = lambda_reg * l2_penalty

    # Initial heat at t = 0
    t_zero = torch.zeros_like(t)
    u_init = model(x, y, t_zero)
    total_heat_init = torch.mean(u_init)
    u_t = model(x, y, t)
    total_heat_t = torch.mean(u_t)

    # Mass conservation loss (heat difference squared)
    loss_mass = lambda_m * (total_heat_t - total_heat_init) ** 2

    return loss_eq + loss_bc + loss_ic + loss_reg + loss_mass


# Training loop
def train_pinn(model, optimizer, x_train, y_train, t_train, x_bc, y_bc, u_bc, x_ic, y_ic, u_ic, alpha, epochs=5000):
    losses = []
    for epoch in range(epochs):
        optimizer.zero_grad()
        loss = heat_loss_fn(model, x_train, y_train, t_train, alpha, x_bc, y_bc, u_bc, x_ic, y_ic, u_ic)
        loss.backward()
        optimizer.step()
        losses.append(loss.item())
        if epoch % 500 == 0:
            print(f"Epoch {epoch}, Loss: {loss.item():.6f}")
    return losses

model = HeatPINN2D()
optimizer = torch.optim.Adam(model.parameters(), lr=0.01)

# Train model
losses = train_pinn(model, optimizer,
                    x_train, y_train, t_train,
                    x_bc, y_bc, u_bc,
                    x_ic, y_ic, u_ic,
                    alpha, epochs=5000)

# Predict
with torch.no_grad():
    u_pred = model(x_train, y_train, t_train)

def plot_results(x_train, y_train, t_train, u_pred, losses):
    plt.figure(figsize=(14, 5))


    # Log Loss curve
    plt.subplot(1, 2, 1)
    plt.plot(losses, label="Loss")
    plt.yscale("log")
    plt.xlabel("Epoch")
    plt.ylabel("Log Loss")
    plt.title("Log Training Loss Curve")
    plt.legend()

    # Predicted vs actual surface temps
    plt.subplot(1, 2, 2)
    plt.scatter(t_train.detach().numpy(), u_pred.detach().numpy(), color='blue', label= "Predicted", alpha=0.4)
    plt.scatter(t_train.detach().numpy(), temps_tensor.detach().numpy(), color='red', label= "Actual",alpha=0.4)
    plt.xlabel("Time (t)")
    plt.ylabel("Temperature u(x, y, t)")
    plt.title("Predicted and Actual Surface Temps over Time")
    plt.legend()

# Plot the results
plot_results(x_train, y_train, t_train, u_pred, losses)

plt.subplot(1, 1, 1)
plt.plot(losses, label = "Loss")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.title("Training Loss Curve")
plt.legend()

x_max = 30.18  # from Width in mm
y_max = 15.5  # from Thickness in mm
t_plot = 360  # In mins

grid_size = 100
x_vals = torch.linspace(0, x_max, grid_size)
y_vals = torch.linspace(0, y_max, grid_size)
x_mesh, y_mesh = torch.meshgrid(x_vals, y_vals, indexing='ij')

# Repeat t value
t_val = torch.full_like(x_mesh, t_plot)

# Flatten for model input
x_input = x_mesh.flatten().unsqueeze(1)
y_input = y_mesh.flatten().unsqueeze(1)
t_input = t_val.flatten().unsqueeze(1)

# Predict
with torch.no_grad():
    u_pred_grid = model(x_input, y_input, t_input).reshape(grid_size, grid_size)

# Plot
plt.figure(figsize=(8, 6))
plt.imshow(u_pred_grid.numpy().T, origin='lower', cmap='inferno', aspect='auto',
           extent=[0, x_max, 0, y_max])
plt.colorbar(label="Temperature (°C)")
plt.xlabel("width (mm)")
plt.ylabel("thickness (mm)")
plt.title(f"Predicted Temperature Distribution at t = {t_plot} min")
plt.tight_layout()
plt.show()